![Metrics](https://raw.githubusercontent.com/phixion/phixion/master/metrics.svg)

## hibp

<!--
for https://github.com/phixion/phixion/blob/main/.github/workflows/feeds.yml
-->
<!--START_SECTION:haveibeenpwnd-->
- Dec 09 - [Kaneva - 3,901,179 breached accounts](https://haveibeenpwned.com/PwnedWebsites#Kaneva)
- Dec 09 - [Gemplex - 4,563,166 breached accounts](https://haveibeenpwned.com/PwnedWebsites#Gemplex)
- Dec 08 - [Movie Forums - 39,914 breached accounts](https://haveibeenpwned.com/PwnedWebsites#MovieForums)
- Dec 07 - [JoyGames - 4,461,787 breached accounts](https://haveibeenpwned.com/PwnedWebsites#JoyGames)
- Dec 05 - [RailYatri - 23,209,732 breached accounts](https://haveibeenpwned.com/PwnedWebsites#RailYatri)
<!--END_SECTION:haveibeenpwnd-->

## hn

<!--
for https://github.com/phixion/phixion/blob/main/.github/workflows/feeds.yml
-->
<!--START_SECTION:hn-->
- Dec 24 - [Mexico is now the 12th largest economy in the world](https://mexiconewsdaily.com/business/mexico-is-now-the-12th-largest-economy-in-the-world/)
- Dec 24 - [The History of Xenix](https://www.abortretry.fail/p/the-history-of-xenix)
- Dec 24 - [Time Is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/abs/2312.13401)
- Dec 24 - [How the weak win wars: A theory of asymmetric conflict (2021) [pdf]](https://web.archive.org/web/20210827053020id_/https://watermark.silverchair.com/016228801753212868.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAtAwggLMBgkqhkiG9w0BBwagggK9MIICuQIBADCCArIGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMsFx_L4GNlU2N202gAgEQgIICg0JcfB_c0FpMcWGV1VmX2fx4NNs5ndOi5wzFx9l8AWBAd_KVbp5AuElpYXEUZSE8F28zUGJJ_pUhQ4dEjJfL8dH2EjaBPxsbstLCOyMPeXUuRQmDLLAPu5_cFf_NlscYvuanjGvnS3TpcYzjA6FHJaP9Y9OnjKkDpejzsEQeFy_aBOGujlgKU94alIeBgxrvxgXAFpLfHYsk3F4LNvKuWPw-AFvADltfKSnF77-4hJFj2oJOUsKSWIiLYmGnPRbP_kvawXfdyhg5W_Rt8KogNPKq4oLodfOokuGEoVdzLEmaX0FcOv_0Zz5LTrbWVG0NufNTglqp6XG3bG29XcXq-Zn5h4QzCudPhaj0_wps0WCgzM5SUjeGevsbRmLtkRHAY2xYrQDivd8RmF7cw3tucjBDxt8l-11KEEp9j5UANFhn5lQDObRsQcbaBGdNXO1SrYRwblxUFzSqgz160x05gFfdfipS1-xxbq2appUBYwS72GmFFROzKRToZy4MbNliSX-KCqXhJW6a6wz4RaL9z-38Xp0pIeYWn1pibF7vj6WF6eLSAU6u-95ufNY7o4aL83JJuhPJLeaowmfWnF4TsOaPNcYr6CfYgh0WAd-SK7gjoAxkZXWV9QpgOInzhgT9DNiD6CxK3UjJRQPxCyQrGUehvwRKmIwMbLrwViTGU60fJcCJxl-ZYbCsUAD8LqRnsDeMY40DE0h9pBQ-AlS5GEfwHmsKXiPgx04LEBz97wt-bEjL6RKBTrzGftTNlHPWpRXQ2QNGMVTtzSyUNUU-3Ci43zjcila8kT298HrAc8wGbX3gWzPMiEGBD8h4bCxToe8xLBH2NZ1jlsETYAsUqWJc9Ds)
- Dec 24 - ["Attention", "Transformers", in Neural Network "Large Language Models"](http://bactra.org/notebooks/nn-attention-and-transformers.html)
<!--END_SECTION:hn-->

<!--
for https://yhype.me
-->
![](https://hit.yhype.me/github/profile?user_id=13013670)
